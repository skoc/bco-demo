{
  "bco_spec_version": "https://w3id.org/biocompute/1.3.0/",
  "bco_id": "http://biocompute.sbgenomics.com/bco/b8684a70-224f-4545-ac71-ba8f35aa70c1",
  "checksum": "473ba083ae8dd7e68b4c2e920b4f0185388d94612c2ef3d5a408231fa5f1f95d",
  "provenance_domain": {
    "name": "Whole Exome Sequencing GATK 2.3.9.-lite",
    "version": "1.0.0",
    "review": [
      {
        "status": "in-review",
        "reviewer_comment": "we have an issue",
        "date": 18144,
        "reviewer": [
          {
            "reviewer_name": "Soner",
            "reviewer_affiliation": "SBG",
            "reviewer_email": "",
            "reviewer_contribution": "",
            "reviewer_orcid": ""
          }
        ]
      }
    ],
    "derived_from": "https://api.sbgenomics.com/v2/apps/soner/soners-demo-project/whole-exome-sequencing-gatk-2-3-9-lite/0/raw/",
    "obsolete_after": "2019-09-05T00:00:00+0000",
    "embargo": ["2019-09-05T00:00:00+0000", "2019-09-05T00:00:00+0000"],
    "created": "2019-09-05T00:00:00+0000",
    "modified": "2019-09-16T00:00:00+0000",
    "contributors": [],
    "license": "https://spdx.org/licenses/CC-BY-4.0.html"
  },
  "usability_domain": "WES pipeline analyzes all protein-coding genes in a genome (known as Exome). The exome is estimated to comprise ~1–2% of the genome, yet contains ~85% of recognized disease-causing mutations. Exome sequencing achieves better coverage compared to whole genome sequencing but is limited to variations affecting coding regions of the genes, thus leaving potential effects on regulatory regions and various control mechanisms undetected. This characteristic determines the applicability of the pipeline to areas where changes in proteins are expected with greater probability or are of higher significance. For example, WES can be used for detecting variants (i.e. mutations) in known disease-causing genes as well as for detection of novel gene-disease associations (H L Rehm, S J Bale et al. ACMG clinical laboratory standards for next-generation sequencing, Genet Med. 2013 September ; 15(9): 733–747. doi:10.1038/gim.2013.92.). \nThe pipeline is constructed following the Broad Institute best practice and utilizing Broad Institute's GATK tools. A separate step is undertaken to assess the quality of sequenced reads using Babraham Institute's tool FastQC. \nSequenced reads are aligned with the BWA tool after which duplicates are removed. The next step uses algorithms developed by the Broad Institute to improve alignment around indels followed by the re-evaluation of the qualities of sequenced bases. Generated SAM files are pooled together and joint variant calling is performed. Detected variants are subjected to additional analysis resulting in refined, high quality set of identified variants (for more information on how variant calling is performed, please refer to the Broad Institute's web site https://www.broadinstitute.org/gatk/guide/topic?name=methods). \nThe pipeline utilizes human reference genome hg19 as well as several public databases (dbSNP v142 and database of known indels generated by Seven Bridges).",
  "extension_domain": {
    "fhir_extension": {
      "fhir_endpoint": "",
      "fhir_version": "",
      "fhir_resources": {}
    },
    "scm_extension": {
      "scm_repository": "",
      "scm_type": "git",
      "scm_commit": "",
      "scm_path": "",
      "scm_preview": ""
    }
  },
  "description_domain": {
    "keywords": [],
    "xref": [],
    "platform": "Seven Bridges Platform",
    "pipeline_steps": [
      {
        "step_number": "1",
        "name": "#SBG_FASTQ_Quality_Converter",
        "description": "This app converts FASTQ quality scores from a given, source format, to standard Sanger quality scores. Supported source formats are: Solexa, Illumina 1.3, Illumina 1.5 and Illumina 1.8.",
        "version": null,
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "2",
        "name": "#SBG_FASTQ_Quality_Detector",
        "description": "FASTQ Quality Scale Detector detects which quality encoding scheme was used in your reads and automatically enters the proper value in the \"Quality Scale\" metadata field.",
        "version": null,
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "3",
        "name": "#FastQC",
        "description": "FastQC reads a set of sequence files and produces a quality control (QC) report from each one. These reports consist of a number of different modules, each of which will help identify a different type of potential problem in your data.\n\nFastQC is a tool which takes a FastQ file and runs a series of tests on it to generate a comprehensive QC report.  This report will tell you if there is anything unusual about your sequence.  Each test is flagged as a pass, warning, or fail depending on how far it departs from what you would expect from a normal large dataset with no significant biases.  It is important to stress that warnings or even failures do not necessarily mean that there is a problem with your data, only that it is unusual.  It is possible that the biological nature of your sample means that you would expect this particular bias in your results.",
        "version": "0.11.4",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "4",
        "name": "#SBG_Html2b64",
        "description": "Tool for converting archived html output of FastQC and similar tools to b64html so it can easily be displayed in web browsers or on SBG platform.",
        "version": "1.0",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "5",
        "name": "#GATK_RealignerTargetCreator",
        "description": "Overview\n\nThe local realignment process is designed to consume one or more BAM files and to locally realign reads such that the number of mismatching bases is minimized across all the reads. In general, a large percent of regions requiring local realignment are due to the presence of an insertion or deletion (indels) in the individual's genome with respect to the reference genome. Such alignment artifacts result in many bases mismatching the reference near the misalignment, which are easily mistaken as SNPs. Moreover, since read mapping algorithms operate on each read independently, it is impossible to place reads on the reference genome such that mismatches are minimized across all reads. Consequently, even when some reads are correctly mapped with indels, reads covering the indel near just the start or end of the read are often incorrectly mapped with respect the true indel, also requiring realignment. Local realignment serves to transform regions with misalignments due to indels into clean reads containing a consensus indel suitable for standard variant discovery approaches. Unlike most mappers, this tool uses the full alignment context to determine whether an appropriate alternate reference (i.e. indel) exists.\n\nThere are 2 steps to the realignment process:\nDetermining (small) suspicious intervals which are likely in need of realignment (RealignerTargetCreator)\nRunning the realigner over those intervals (see the IndelRealigner tool)\nFor more details, see the indel realignment method documentation.\n\nInputs\nOne or more aligned BAM files and optionally, one or more lists of known indels.\n\nOutput\nA list of target intervals to pass to the IndelRealigner.\n\nUsage example:\n java -jar GenomeAnalysisTK.jar \\\n   -T RealignerTargetCreator \\\n   -R reference.fasta \\\n   -I input.bam \\\n   --known indels.vcf \\\n   -o forIndelRealigner.intervals\n \nNotes\n\nThe input BAM(s), reference, and known indel file(s) should be the same ones to be used for the IndelRealigner step.\nWhen multiple potential indels are found by the tool in the same general region, the tool will choose the most likely one for realignment to the exclusion of the others. This is a known limitation of the tool.\nBecause reads produced from the 454 technology inherently contain false indels, the realigner will not work with them (or with reads from similar technologies).\nThis tool also ignores MQ0 reads and reads with consecutive indel operators in the CIGAR string.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "6",
        "name": "#GATK_IndelRealigner",
        "description": "Overview\n\nThe local realignment process is designed to consume one or more BAM files and to locally realign reads such that the number of mismatching bases is minimized across all the reads. In general, a large percent of regions requiring local realignment are due to the presence of an insertion or deletion (indels) in the individual's genome with respect to the reference genome. Such alignment artifacts result in many bases mismatching the reference near the misalignment, which are easily mistaken as SNPs. Moreover, since read mapping algorithms operate on each read independently, it is impossible to place reads on the reference genome such at mismatches are minimized across all reads. Consequently, even when some reads are correctly mapped with indels, reads covering the indel near just the start or end of the read are often incorrectly mapped with respect the true indel, also requiring realignment. Local realignment serves to transform regions with misalignments due to indels into clean reads containing a consensus indel suitable for standard variant discovery approaches. Unlike most mappers, this walker uses the full alignment context to determine whether an appropriate alternate reference (i.e. indel) exists. Following local realignment, the GATK tool Unified Genotyper can be used to sensitively and specifically identify indels.\n\nThere are 2 steps to the realignment process:\n\n1. Determining (small) suspicious intervals which are likely in need of realignment (see the RealignerTargetCreator tool)\n2. Running the realigner over those intervals (IndelRealigner)\nFor more details, see the indel realignment method documentation.\n\nInput\nOne or more aligned BAM files and optionally one or more lists of known indels.\n\nOutput\nA realigned version of your input BAM file(s).\n\nUsage example:\n java -jar GenomeAnalysisTK.jar \\\n   -T IndelRealigner \\\n   -R reference.fasta \\\n   -I input.bam \\\n   --known indels.vcf \\\n   -targetIntervals intervalListFromRTC.intervals \\\n   -o realignedBam.bam\n \nCaveats\n\nThe input BAM(s), reference, and known indel file(s) should be the same ones to be used for the IndelRealigner step.\nBecause reads produced from the 454 technology inherently contain false indels, the realigner will not work with them (or with reads from similar technologies).\nThis tool also ignores MQ0 reads and reads with consecutive indel operators in the CIGAR string.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "7",
        "name": "#GATK_BaseRecalibrator",
        "description": "Overview\n\nThis tool is designed to work as the first pass in a two-pass processing step. It does a by-locus traversal operating only at sites that are not in dbSNP. We assume that all reference mismatches we see are therefore errors and indicative of poor base quality. This tool generates tables based on various user-specified covariates (such as read group, reported quality score, cycle, and context). Since there is a large amount of data, one can then calculate an empirical probability of error given the particular covariates seen at this site, where p(error) = num mismatches / num observations. The output file is a table (of the several covariate values, num observations, num mismatches, empirical quality score).\n\nNote: ReadGroupCovariate and QualityScoreCovariate are required covariates and will be added regardless of whether or not they were specified.\n\nInput\nA BAM file containing data that needs to be recalibrated.\nA database of known polymorphic sites to mask out.\n\nOutput\nA GATKReport file with many tables:\nThe list of arguments\nThe quantized qualities table\nThe recalibration table by read group\nThe recalibration table by quality score\nThe recalibration table for all the optional covariates\nThe GATKReport table format is intended to be easy to read by both humans and computer languages (especially R). Check out the documentation of the GATKReport (in the FAQs) to learn how to manipulate this table.\n\nUsage example\n java -jar GenomeAnalysisTK.jar \\\n   -T BaseRecalibrator \\\n   -R reference.fasta \\\n   -I my_reads.bam \\\n   -knownSites latest_dbsnp.vcf \\\n   -o recal_data.table\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "8",
        "name": "#GATK_PrintReads",
        "description": "Overview\n\nPrintReads is a generic utility tool for manipulating sequencing data in SAM/BAM format. It can dynamically merge the contents of multiple input BAM files, resulting in merged output sorted in coordinate order. It can also optionally filter reads based on various read properties such as read group tags using the `--read_filter/-rf` command line argument (see documentation on read filters for more information).\n\nNote that when PrintReads is used as part of the Base Quality Score Recalibration workflow, it takes the `--BQSR` engine argument, which is listed under Inherited Arguments > CommandLineGATK below.\n\nInput\nOne or more bam files.\n\nOutput\nA single processed bam file.\n\nUsage examples:\n\n // Prints all reads that have a mapping quality above zero\n java -jar GenomeAnalysisTK.jar \\\n   -T PrintReads \\\n   -R reference.fasta \\\n   -I input1.bam \\\n   -I input2.bam \\\n   -o output.bam \\\n   --read_filter MappingQualityZero\n\n // Prints the first 2000 reads in the BAM file\n java -jar GenomeAnalysisTK.jar \\\n   -T PrintReads \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o output.bam \\\n   -n 2000\n\n // Downsamples BAM file to 25%\n java -jar GenomeAnalysisTK.jar \\\n   -T PrintReads \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o output.bam \\\n   -dfrac 0.25\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "9",
        "name": "#Picard_BuildBamIndex",
        "description": "Picard BuildBamIndex generates a BAM index (.bai) file.",
        "version": "1.140",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "10",
        "name": "#Picard_CollectAlignmentSummaryMetrics",
        "description": "Picard CollectAlignmentSummaryMetrics assesses the quality of alignment by analyzing a SAM or BAM file. It compares it with the reference file (FASTA) and provides alignment statistics, such as the number of input reads and the percent of reads that are mapped. It produces a file which contains summary alignment metrics from a SAM or BAM file.\n\nNote: This tool requires the exact same FASTA file as the one to which raw reads were aligned.",
        "version": "1.140",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "11",
        "name": "#GATK_UnifiedGenotyper",
        "description": "Overview\n\nThis tool uses a Bayesian genotype likelihood model to estimate simultaneously the most likely genotypes and allele frequency in a population of N samples, emitting a genotype for each sample. The system can either emit just the variant sites or complete genotypes (which includes homozygous reference calls) satisfying some phred-scaled confidence value.\n\nInput\nThe read data from which to make variant calls.\n\nOutput\nA raw, unfiltered, highly sensitive callset in VCF format.\n\nUsage examples:\n\n//Multi-sample SNP calling\n java -jar GenomeAnalysisTK.jar \\\n   -T UnifiedGenotyper \\\n   -R reference.fasta \\\n   -I sample1.bam [-I sample2.bam ...] \\\n   --dbsnp dbSNP.vcf \\\n   -o snps.raw.vcf \\\n   -stand_call_conf [50.0] \\\n   -stand_emit_conf 10.0 \\\n   [-L targets.interval_list]\n \n//Generate calls at all sites\n java -jar GenomeAnalysisTK.jar \\\n   -T UnifiedGenotyper \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o raw_variants.vcf \\\n   --output_mode EMIT_ALL_SITES\n \nCaveats\n\nThe caller can be very aggressive in calling variants in order to be very sensitive, so the raw output will contain many false positives. We use extensive post-calling filters to eliminate most of these FPs. See the documentation on filtering (especially by Variant Quality Score Recalibration) for more details.\nThis tool has been deprecated in favor of HaplotypeCaller, a much more sophisticated variant caller that produces much better calls, especially on indels, and includes features that allow it to scale to much larger cohort sizes.\nSpecial note on ploidy\n\nThis tool is able to handle almost any ploidy (except very high ploidies in large pooled experiments); the ploidy can be specified using the -ploidy argument for non-diploid organisms.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "12",
        "name": "#GATK_VariantFiltration_",
        "description": "Overview\nThis tool is designed for hard-filtering variant calls based on certain criteria. Records are hard-filtered by changing the value in the FILTER field to something other than PASS. Filtered records will be preserved in the output unless their removal is requested in the command line.\n\nInput\nA variant set to filter.\n\nOutput\nA filtered VCF.\n\nUsage example\n java -jar GenomeAnalysisTK.jar \\\n   -T VariantFiltration \\\n   -R reference.fasta \\\n   -o output.vcf \\\n   --variant input.vcf \\\n   --filterExpression \"AB < 0.2 || MQ0 > 50\" \\\n   --filterName \"Nov09filters\" \\\n   --mask mask.vcf \\\n   --maskName InDel\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "13",
        "name": "#GATK_VariantFiltration__1",
        "description": "Overview\nThis tool is designed for hard-filtering variant calls based on certain criteria. Records are hard-filtered by changing the value in the FILTER field to something other than PASS. Filtered records will be preserved in the output unless their removal is requested in the command line.\n\nInput\nA variant set to filter.\n\nOutput\nA filtered VCF.\n\nUsage example\n java -jar GenomeAnalysisTK.jar \\\n   -T VariantFiltration \\\n   -R reference.fasta \\\n   -o output.vcf \\\n   --variant input.vcf \\\n   --filterExpression \"AB < 0.2 || MQ0 > 50\" \\\n   --filterName \"Nov09filters\" \\\n   --mask mask.vcf \\\n   --maskName InDel\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "14",
        "name": "#GATK_SelectVariants",
        "description": "Overview\n\nOften, a VCF containing many samples and/or variants will need to be subset in order to facilitate certain analyses (e.g. comparing and contrasting cases vs. controls; extracting variant or non-variant loci that meet certain requirements, displaying just a few samples in a browser like IGV, etc.). SelectVariants can be used for this purpose.\n\nThere are many different options for selecting subsets of variants from a larger callset:\n\nExtract one or more samples from a callset based on either a complete sample name or a pattern match.\nSpecify criteria for inclusion that place thresholds on annotation values, e.g. \"DP > 1000\" (depth of coverage greater than 1000x), \"AF < 0.25\" (sites with allele frequency less than 0.25). These criteria are written as \"JEXL expressions\", which are documented in the article about using JEXL expressions.\nProvide concordance or discordance tracks in order to include or exclude variants that are also present in other given callsets.\nSelect variants based on criteria like their type (e.g. INDELs only), evidence of mendelian violation, filtering status, allelicity, and so on.\nThere are also several options for recording the original values of certain annotations that are recalculated when a subsetting the new callset, trimming alleles, and so on.\n\nInput\nA variant call set from which to select a subset.\n\nOutput\nA new VCF file containing the selected subset of variants.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "15",
        "name": "#GATK_SelectVariants_1",
        "description": "Overview\n\nOften, a VCF containing many samples and/or variants will need to be subset in order to facilitate certain analyses (e.g. comparing and contrasting cases vs. controls; extracting variant or non-variant loci that meet certain requirements, displaying just a few samples in a browser like IGV, etc.). SelectVariants can be used for this purpose.\n\nThere are many different options for selecting subsets of variants from a larger callset:\n\nExtract one or more samples from a callset based on either a complete sample name or a pattern match.\nSpecify criteria for inclusion that place thresholds on annotation values, e.g. \"DP > 1000\" (depth of coverage greater than 1000x), \"AF < 0.25\" (sites with allele frequency less than 0.25). These criteria are written as \"JEXL expressions\", which are documented in the article about using JEXL expressions.\nProvide concordance or discordance tracks in order to include or exclude variants that are also present in other given callsets.\nSelect variants based on criteria like their type (e.g. INDELs only), evidence of mendelian violation, filtering status, allelicity, and so on.\nThere are also several options for recording the original values of certain annotations that are recalculated when a subsetting the new callset, trimming alleles, and so on.\n\nInput\nA variant call set from which to select a subset.\n\nOutput\nA new VCF file containing the selected subset of variants.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "16",
        "name": "#GATK_CombineVariants",
        "description": "Overview\n\nCombineVariants reads in variants records from separate ROD (Reference-Ordered Data) sources and combines them into a single VCF. Any (unique) name can be used to bind your ROD and any number of sources can be input. This tool aims to fulfill two main possible use cases, reflected by the two combination options (MERGE and UNION), for merging records at the variant level (the first 8 fields of the VCF) or at the genotype level (the rest).\n\nMERGE: combines multiple variant records present at the same site in the different input sources into a single variant record in the output. If sample names overlap, then they are \"uniquified\" by default, which means a suffix is appended to make them unique. Note that in version 3.3, the automatic uniquifying was disabled (unintentionally), and required setting `-genotypeMergeOptions UNIQUIFY` manually.\nUNION: assumes that each ROD source represents the same set of samples (although this is not enforced). It uses the priority list (if provided) to emit a single record instance at every position represented in the input RODs.\nCombineVariants will emit a record for every site that was present in any of your input VCF files, and will annotate (in the set attribute in the INFO field) whether the record had a PASS or FILTER status in each input ROD . In effect, CombineVariants always produces a union of the input VCFs. However, any part of the Venn of the merged VCFs can be extracted using JEXL expressions on the set attribute using SelectVariants. If you want to extract just the records in common between two VCFs, you would first run CombineVariants on the two files to generate a single VCF and then run SelectVariants to extract the common records with `-select 'set == \"Intersection\"'`, as worked out in the detailed example in the documentation guide.\n\nInput\nTwo or more variant sets to combine.\n\nOutput\nA combined VCF.\n\nUsage examples\n\nMerge two separate callsets\n java -jar GenomeAnalysisTK.jar \\\n   -T CombineVariants \\\n   -R reference.fasta \\\n   --variant input1.vcf \\\n   --variant input2.vcf \\\n   -o output.vcf \\\n   -genotypeMergeOptions UNIQUIFY\n \nGet the union of calls made on the same samples\n java -jar GenomeAnalysisTK.jar \\\n   -T CombineVariants \\\n   -R reference.fasta \\\n   --variant:foo input1.vcf \\\n   --variant:bar input2.vcf \\\n   -o output.vcf \\\n   -genotypeMergeOptions PRIORITIZE \\\n   -priority foo,bar\n \nCaveats\n\nThis tool is not intended to manipulate GVCFS! To combine GVCF files output by HaplotypeCaller, use CombineGVCFs.\nTo join intermediate VCFs produced by running jobs in parallel by interval (e.g. by chromosome), use CatVariants.\n\nAdditional notes\n\nUsing this tool's multi-threaded parallelism capability is particularly useful when converting from VCF to BCF2, which can be time-consuming. In this case each thread spends CPU time doing the conversion, and the GATK engine is smart enough to merge the partial BCF2 blocks together efficiently. However, since this merge runs in only one thread, you can quickly reach diminishing returns with the number of parallel threads. In our hands, `-nt 4` works well but `-nt 8` tends to be be too much.\nSince GATK 2.1, when merging multiple VCF records at a site, the combined VCF record has the QUAL of the first VCF record with a non-MISSING QUAL value. The previous behavior was to take the max QUAL, which could result in strange downstream confusion.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.",
        "version": "2.3.9 Lite",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "17",
        "name": "#SnpEff_adjusted",
        "description": "SnpEff Adjusted is a variant annotation and effect-prediction tool. It annotates and predicts the effects of variants on genes, such as amino acid changes.\n\nTypical usage assumes the user chooses inputs that are predicted variants (SNPs, insertions, deletions, and MNPs). This input file is usually the result of a sequencing experiment, and it is usually in variant call format (VCF). SnpEff analyzes the input variants and, in the process, it annotates the variants and calculates the effects they produce on known genes (e.g. amino acid changes). The output file can be in several file formats. The most common format is VCF.\n\nThis is an adjusted version of the tool that primarily handles config file editing. There is also a command line option to control the amount of RAM in MB [-Xmx%m] for java, which is a custom parameter.",
        "version": "3.6",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      },
      {
        "step_number": "18",
        "name": "#BWA_MEM_Bundle",
        "description": "**BWA-MEM** is an algorithm designed for aligning sequence reads onto a large reference genome. BWA-MEM is implemented as a component of BWA. The algorithm can automatically choose between performing end-to-end and local alignments. BWA-MEM is capable of outputting multiple alignments, and finding chimeric reads. It can be applied to a wide range of read lengths, from 70 bp to several megabases. \n\nBesides the standard BWA-MEM SAM output file, the App has been extended to support two additional output options: a BAM file obtained by piping through **Sambamba view** while filtering out the secondary alignments, as well as a Coordinate Sorted BAM option that additionally pipes the output through **Sambamba Sort**, along with an accompanying bai file produced by **Sambamba Index**. Parameters responsible for these additional features are 'Filter out secondary alignments' and 'Output format'.\n\nIf deduplication is needed, it can be done by setting the parameter 'Duplication'. **Samblaster** will be used internally to perform this action.\n\nFor fastq files of total size less than 10GB we suggest using the default setting for Amount of Total memory of 15GB, for larger files we suggest using 58 GB of memory (and 32 CPU cores) [Default: 15]",
        "version": "0.7.12",
        "prerequisite": [],
        "input_list": [],
        "output_list": []
      }
    ]
  },
  "execution_domain": {
    "script": "https://api.sbgenomics.com/v2/apps/soner/soners-demo-project/whole-exome-sequencing-gatk-2-3-9-lite/0/raw/",
    "script_driver": "Seven Bridges Common Workflow Language Executor",
    "software_prerequisites": [],
    "external_data_endpoints": [],
    "environment_variables": []
  },
  "parametric_domain": [],
  "io_domain": {
    "input_subdomain": [
      {
        "uri": [
          {
            "filename": "dbsnp_137.b37.vcf",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f2065c8",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "1000G_phase1.indels.b37.vcf",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206662",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "Mills_and_1000G_gold_standard.indels.b37.sites.vcf",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206642",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "exome_targets.b37.bed",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f20663d",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "snpEff_v3_6_GRCh37.75.zip",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f2065f0",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "human_g1k_v37_decoy.fasta",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f20667f",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "WES_Targeted_human_Illumina.pe_1.fastq",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206535",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "uri": [
          {
            "filename": "WES_Targeted_human_Illumina.pe_2.fastq",
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206565",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      }
    ],
    "output_subdomain": [
      {
        "mediatype": "html",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206590",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "bai",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206524",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "bai",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/56f4217de4b02acf3b2dd766",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "txt",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206605",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "vcf",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206576",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "idx",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/56f42b38e4b0f46590c9ac2d",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "sh",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f20662a",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "txt",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f20654f",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "pdf",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206569",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "b64html",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206522",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "b64html",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206549",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "html",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f20651a",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "html",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f206625",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      },
      {
        "mediatype": "vcf",
        "uri": [
          {
            "uri": "https://igor.sbgenomics.com/u/soner/soners-demo-project/files/58063598e4b0c58b0f2065a7",
            "access_time": "2016-10-18T14:45:49Z"
          }
        ]
      }
    ]
  },
  "error_domain": {
    "empirical_error": [],
    "algorithmic_error": []
  }
}
